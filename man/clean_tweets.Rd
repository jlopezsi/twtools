% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Functions.R
\name{clean_tweets}
\alias{clean_tweets}
\title{Clean Tweets}
\usage{
clean_tweets(tweets, reg = "([^A-Za-z\\\\d#@']|'(?![A-Za-z\\\\d#@]))",
  hashtags = "trim", remove.mentions = TRUE)
}
\arguments{
\item{tweets}{An input dataset of raw tweets, usually from searchTwitter()}

\item{reg}{This is the rule by which tweets are split into tokens. It uses regex (regular expressions), and by default
removes all punctuation except #, @ and ', then splits words wherever there is white space between them. Adding symbols
to the first part of the expression will mean they are retained (for instance [^A-Za-z\\!\\d#@'] will keep exclamation 
points)}

\item{hashtags}{is set to 'trim' by default, which keeps them but removes the #. If set to 'remove', they will be removed
entirely and so not affect the word count. If set to 'keep', they will be kept but not trimmed and so will not match
most dictionaries, unless the dictionary software removes punctuation by default.}

\item{remove.mentions}{is set to TRUE by default, removing mentions (e.g. @seanchrismurphy). It can be set to FALSE to 
retain the mentions.}
}
\description{
This function takes a dataframe of raw tweets and performs some basic cleaning and tokenization. It returns a 
dataframe with each tweet broken into individual word tokens, with one row for each word. Can then be used as 
input to dictionary_count.
}

